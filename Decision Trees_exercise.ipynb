{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|--|--|\n",
    "| **Names** | *Andrea Papenmeier, Waheed ud din Siddiqui* |\n",
    "| **Group** | *ML_HMI 02* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "In this lab, we will implement the C4.5 algorithm for classification with decision trees. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Let's start by loading the datasets that we will be working with. The ```mushroom``` dataset is a dataset describing the features of different species of mushrooms, and the goal is to derive simple rules to predict whether a species is poisonous or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inputs', 'targets', 'feature_names']\n",
      "8416\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "data = np.load(\"uci-mushrooms.npz\")\n",
    "print data.keys()\n",
    "\n",
    "d = data['inputs']\n",
    "l = data['targets']\n",
    "fname = data['feature_names']\n",
    "print(l.size)\n",
    "#check = []\n",
    "#for i in range(fname.shape[0]):\n",
    "#    check.append(np.sum(d[:,i] == '?'))\n",
    "#print(check)\n",
    "#print(d[:,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Here are a few functions to deal with categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3928\n",
      "4488\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "\n",
    "def listValues(inputlist):\n",
    "    \"\"\"List the possible values of a given feature in a dataset\"\"\"\n",
    "    values = set([])\n",
    "    for x in inputlist:\n",
    "        values.add(x)\n",
    "    return list(values)\n",
    "\n",
    "def countLabels(labelList):\n",
    "    \"\"\"Return the number of labels in a list\"\"\"\n",
    "    counts = {}\n",
    "    for x in labelList:\n",
    "        if x not in counts:\n",
    "            counts[x] = 1\n",
    "        else:\n",
    "            counts[x] += 1\n",
    "    return counts\n",
    "\n",
    "def majorityLabel(labelList):\n",
    "    counts = countLabels(labelList)\n",
    "#    print \"Majority label:\", counts\n",
    "    return max(counts.iteritems(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "for value in listValues(l):\n",
    "    targetIndicesTrue  = l == value\n",
    "    print(np.sum(targetIndicesTrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996803828522\n"
     ]
    }
   ],
   "source": [
    "# This assignment was somewhat unclear, we understood it such that we should calculate the entropy of a dataset\n",
    "# given the class labels (slide 12 of lecture slides week 6)\n",
    "\n",
    "def entropy(labelList):\n",
    "    \n",
    "    # count all elements\n",
    "    nbrTotalElements = len(labelList)\n",
    "    \n",
    "    # count instances in class 1\n",
    "    nbrClass1 = countLabels(labelList)[(listValues(labelList)[0])]\n",
    "    # count instances in class 2\n",
    "    nbrClass2 = nbrTotalElements-nbrClass1\n",
    "    if ((nbrClass1 == nbrTotalElements) or (nbrClass2 == nbrTotalElements)):\n",
    "        res = 0\n",
    "    else:\n",
    "        res = 0-(((nbrClass1+0.0)/nbrTotalElements)*np.log2(((nbrClass1+0.0)/nbrTotalElements)))-(((nbrClass2+0.0)/nbrTotalElements)*np.log2(((nbrClass2+0.0)/nbrTotalElements)))\n",
    "    \n",
    "    return res\n",
    "\n",
    "print entropy(l)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the entropy of a list of categorical values\n",
    "\n",
    "The decision tree makes its decision based on decision nodes, and these nodes are evaluated based on information gain they provide on the training data. Information gain is computed as \n",
    "$$\n",
    "IG = H(parent) - \\sum_j \\frac{N_j}{N} H(child=j)\n",
    "$$\n",
    "the entropy of the data associated with the parent, minus the weighted entropy of the data-subsets associated with the chidlren\n",
    "\n",
    "**question 1 [5 credits]]** Implement a helper function that computes the entropy of a set of categorical labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the tree\n",
    "\n",
    "Below, you are given the code to make a decision tree, train it and classify datapoints using binary decision nodes based on categorical variables. The implementation includes  ```Leaf``` nodes, wich return a classification result, and ```BinaryNode```, which decide which child should be responsible for the further classification.\n",
    "\n",
    "The ```BinaryNode``` constructor selects the most informative value in the data it's trained on, and makes a decision based on whether or not the feature of the datapoint to be classified equals that value, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "\n",
    "class Node:\n",
    "    def classify(self, datapoint):\n",
    "        \"\"\"Return the classification label for the given datapoint\"\"\"\n",
    "        raise Exception(\"Not Implemented\")        \n",
    "    def entropy(self):\n",
    "        \"\"\"Return the entropy of the training data associated with this node\"\"\"\n",
    "    def informationGain(self):\n",
    "        \"\"\"Return the information gain of this node on the training data \"\"\"\n",
    "        raise Exception(\"Not Implemented by derived class\")\n",
    "\n",
    "class Leaf(Node):\n",
    "    def __init__(self,inputs, targets):\n",
    "        self.v = majorityLabel(targets)\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.children = None\n",
    "        self.H = entropy(targets)\n",
    "        \n",
    "    def entropy(self):\n",
    "        return self.H\n",
    "    \n",
    "    def informationGain(self):\n",
    "        return 0.\n",
    "    \n",
    "    def classify(self,datapoint):\n",
    "        return self.v\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\\t\\t\\t=> %s\" % (self.v)\n",
    "    \n",
    "class BinaryNode(Node):\n",
    "    \"\"\"Make a decision based on whether a given feature equals a given value, or not\"\"\"\n",
    "    def __init__(self, inputs, targets, feature):\n",
    "        self.feature = feature\n",
    "        self.value   = None\n",
    "        self.IG      = 0.0\n",
    "        self.H       = entropy(targets)                     # Entropy of unsplit data\n",
    "        self.inputs  = inputs\n",
    "        self.targets = targets\n",
    "        N = float(targets.size)                             # Total number of datapoints\n",
    "\n",
    "        for value in listValues(inputs[:,feature]):\n",
    "            indicesTrue  = inputs[:,feature]==value\n",
    "            indicesFalse = inputs[:,feature]!=value\n",
    "\n",
    "            if indicesTrue.all() or indicesFalse.all():\n",
    "                continue\n",
    "\n",
    "            children = [ Leaf(inputs[indicesTrue,:], targets[indicesTrue]),\n",
    "                         Leaf(inputs[indicesFalse,:], targets[indicesFalse]) ]\n",
    "            conditions = [ \"%s==%s\" % (fname[feature], value), \"%s!=%s\" % (fname[feature], value) ]\n",
    "\n",
    "            Nt = float(children[0].targets.size)        # N.o. datapoints in True condition\n",
    "            Nf = float(children[1].targets.size)        # N.o. datapoints in False condition\n",
    "            pt = Nt/N\n",
    "            pf = Nf/N\n",
    "            ig = self.H - pt * children[0].entropy() - pf * children[1].entropy()\n",
    "            \n",
    "            if ig > self.IG:\n",
    "                self.value = value\n",
    "                self.IG = ig\n",
    "                self.children = children\n",
    "                self.conditions = conditions\n",
    "    \n",
    "    def entropy(self):\n",
    "        return self.H\n",
    "    \n",
    "    def informationGain(self):\n",
    "        \"\"\"Return the entropy gain of splitting the given data according to \n",
    "        the specified feature and value\"\"\"\n",
    "        return self.IG\n",
    "    \n",
    "    def classify(self, x):\n",
    "        if x[self.feature] == self.value:\n",
    "            return self.children[0].classify(x)\n",
    "        else:\n",
    "            return self.children[1].classify(x)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \" => Binary decision: '%s =?= %s'\" % (fname[self.feature], self.value )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "\n",
    "def c45(inputs, targets, minSize=100):\n",
    "    ig = 0.\n",
    "    res = None\n",
    "    for feature in range(inputs.shape[1]):\n",
    "        n = BinaryNode(inputs,targets,feature)\n",
    "        if n.informationGain() > ig:\n",
    "            ig = n.informationGain()\n",
    "            res = n\n",
    "    if not res:\n",
    "        print \"WARNING: No informative feature found (Fishy!)\"\n",
    "        return res\n",
    "    for i in range(len(res.children)):\n",
    "        if res.children[i].entropy() > 0. and res.children[i].targets.size>minSize:\n",
    "            c = c45(res.children[i].inputs, res.children[i].targets, minSize)\n",
    "            if c:\n",
    "                res.children[i]=c\n",
    "    return res\n",
    "\n",
    "root = c45(d,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+->  => n-ary decision: stalk-shape = ?\n",
      "   +-> stalk-shape==ENLARGING => n-ary decision: gill-size = ?\n",
      "   |  +-> gill-size==BROAD => n-ary decision: gill-attachment = ?\n",
      "   |  |  +-> gill-attachment==ATTACHED \t\t\t=> EDIBLE\n",
      "   |  |  +-> gill-attachment!=FREE => n-ary decision: gill-spacing = ?\n",
      "   |  |     +-> gill-spacing==CLOSE => n-ary decision: Number-of-rings = ?\n",
      "   |  |     |  +-> Number-of-rings==NONE \t\t\t=> POISONOUS\n",
      "   |  |     |  +-> Number-of-rings!=NONE \t\t\t=> EDIBLE\n",
      "   |  |     |  +-> Number-of-rings!=ONE => n-ary decision: cap-shape = ?\n",
      "   |  |     |     +-> cap-shape==FLAT => n-ary decision: habitat = ?\n",
      "   |  |     |     |  +-> habitat==PATHS \t\t\t=> POISONOUS\n",
      "   |  |     |     |  +-> habitat!=PATHS \t\t\t=> POISONOUS\n",
      "   |  |     |     |  +-> habitat!=GRASSES \t\t\t=> POISONOUS\n",
      "   |  |     |     +-> cap-shape!=FLAT => n-ary decision: habitat = ?\n",
      "   |  |     |     |  +-> habitat==GRASSES \t\t\t=> POISONOUS\n",
      "   |  |     |     |  +-> habitat!=GRASSES \t\t\t=> POISONOUS\n",
      "   |  |     |     |  +-> habitat==WOODS \t\t\t=> EDIBLE\n",
      "   |  |     |     |  +-> habitat!=PATHS \t\t\t=> POISONOUS\n",
      "   |  |     |     +-> cap-shape!=BELL \t\t\t=> EDIBLE\n",
      "   |  |     +-> gill-spacing!=CROWDED \t\t\t=> EDIBLE\n",
      "   |  +-> gill-size!=NARROW => n-ary decision: stalk-colour-above-ring = ?\n",
      "   |     +-> stalk-colour-above-ring==WHITE => n-ary decision: gill-spacing = ?\n",
      "   |     |  +-> gill-spacing==CLOSE => n-ary decision: stalk-surface-above-ring = ?\n",
      "   |     |  |  +-> stalk-surface-above-ring==SILKY \t\t\t=> POISONOUS\n",
      "   |     |  |  +-> stalk-surface-above-ring!=SMOOTH \t\t\t=> POISONOUS\n",
      "   |     |  +-> gill-spacing!=CROWDED \t\t\t=> POISONOUS\n",
      "   |     +-> stalk-colour-above-ring!=YELLOW \t\t\t=> POISONOUS\n",
      "   +-> stalk-shape!=TAPERING => n-ary decision: stalk-colour-above-ring = ?\n",
      "      +-> stalk-colour-above-ring==PINK => n-ary decision: cap-shape = ?\n",
      "      |  +-> cap-shape==FLAT => n-ary decision: cap-colour = ?\n",
      "      |  |  +-> cap-colour==BROWN \t\t\t=> POISONOUS\n",
      "      |  |  +-> cap-colour!=BROWN \t\t\t=> POISONOUS\n",
      "      |  |  +-> cap-colour!=GRAY \t\t\t=> EDIBLE\n",
      "      |  +-> cap-shape!=FLAT \t\t\t=> POISONOUS\n",
      "      |  +-> cap-shape!=CONVEX => n-ary decision: cap-colour = ?\n",
      "      |     +-> cap-colour==BROWN \t\t\t=> POISONOUS\n",
      "      |     +-> cap-colour!=BROWN \t\t\t=> POISONOUS\n",
      "      |     +-> cap-colour!=GRAY \t\t\t=> EDIBLE\n",
      "      +-> stalk-colour-above-ring!=PINK \t\t\t=> EDIBLE\n",
      "      +-> stalk-colour-above-ring!=WHITE => n-ary decision: bruises? = ?\n",
      "         +-> bruises?==BRUISES => n-ary decision: cap-shape = ?\n",
      "         |  +-> cap-shape==FLAT \t\t\t=> EDIBLE\n",
      "         |  +-> cap-shape!=CONVEX \t\t\t=> EDIBLE\n",
      "         +-> bruises?!=NO => n-ary decision: cap-shape = ?\n",
      "            +-> cap-shape==FLAT => n-ary decision: stalk colour-below-ring = ?\n",
      "            |  +-> stalk colour-below-ring==PINK \t\t\t=> POISONOUS\n",
      "            |  +-> stalk colour-below-ring!=WHITE => n-ary decision: cap-surface = ?\n",
      "            |     +-> cap-surface==FIBROUS \t\t\t=> EDIBLE\n",
      "            |     +-> cap-surface!=FIBROUS \t\t\t=> EDIBLE\n",
      "            |     +-> cap-surface!=SCALY \t\t\t=> POISONOUS\n",
      "            +-> cap-shape!=FLAT \t\t\t=> POISONOUS\n",
      "            +-> cap-shape!=CONVEX => n-ary decision: stalk colour-below-ring = ?\n",
      "               +-> stalk colour-below-ring==PINK \t\t\t=> POISONOUS\n",
      "               +-> stalk colour-below-ring!=WHITE => n-ary decision: cap-surface = ?\n",
      "                  +-> cap-surface==FIBROUS \t\t\t=> EDIBLE\n",
      "                  +-> cap-surface!=FIBROUS \t\t\t=> EDIBLE\n",
      "                  +-> cap-surface!=SCALY \t\t\t=> POISONOUS\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "\n",
    "def prettyPrint(tree, cond=\"\", dlist = [True]):\n",
    "    indent = \"\"\n",
    "    for d in dlist[:-1]:\n",
    "        indent += \"   \" if d else \"|  \"\n",
    "    print indent + \"+-> \" + cond + \" \" + tree.__str__()\n",
    "    if tree.children:\n",
    "        for cond,cld in zip(tree.conditions[:-1],tree.children[:-1]):\n",
    "            prettyPrint(cld,cond,dlist + [ False ])\n",
    "        prettyPrint(tree.children[-1], tree.conditions[-1], dlist+[ True ])\n",
    "\n",
    "    \n",
    "\n",
    "prettyPrint(root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "The code above trains a decision tree.\n",
    "\n",
    "**Question 2 [5 credits]** Split the ```mushroom``` dataset into 10 folds and perform 10-fold cross-validated training and testing of the decision tree. Report de tree's performance in terms of average precision, recall and accuracy.\n",
    "\n",
    "**Question 3 [5 credits]** Modify the training function provided above to stop splitting nodes when the corresponding training data contains less than ```minSize``` datapoints, where ```minSize``` is a parameter to the function. Split the data into 80\\% train and 20\\% test, and use 10-fold cross-validation on the training set to find a good value for this parameter. Then report the confusion matrix, precision, recall and accuracy that you obtain on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer to Q2\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def kfoldsvalidation(datapoints, labels, foldNbr, minSizeNodes):\n",
    "    \"\"\"function to perform k-fold cross-validation with varying minSize in c45 function\"\"\"\n",
    "    #help(KFold)\n",
    "    # divide dataset into 10 times training+test data\n",
    "    kf = KFold(len(labels), n_folds=foldNbr)\n",
    "    X = datapoints\n",
    "    y = labels\n",
    "\n",
    "    # setup array to store subsets and trees\n",
    "    subsets = []\n",
    "    root = []\n",
    "\n",
    "    # loop through all 10 cross validation sets and store them in array\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        subsets.append(((X_train, y_train), (X_test, y_test)))\n",
    "\n",
    "    #print len(subsets) # == 10 --> we have 10 subsets\n",
    "    #print len(subsets[0]) # == 2 --> we each subset has a subsubset of train data and one of test data\n",
    "    #print len(subsets[0][0]) # == 2 --> each subsubset has a set of labels and a set of 22 feature values\n",
    "    \n",
    "    # construct trees and store in root array\n",
    "    i = 0\n",
    "    while i<len(subsets):\n",
    "        dat = subsets[i][0][0]\n",
    "        lab = subsets[i][0][1]\n",
    "        root.append(c45(dat, lab, minSizeNodes))\n",
    "        i += 1\n",
    "    \n",
    "    # output a set of trees and subsets\n",
    "    return root, subsets\n",
    "\n",
    "roots = kfoldsvalidation(d, l, 10, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averages of all 10 folds together:\n",
      "precision:\n",
      "0.995899238103 1.0\n",
      "recall:\n",
      "1.0 0.996468327916\n",
      "accuracy:\n",
      "0.466730167965\n"
     ]
    }
   ],
   "source": [
    "# Continuation of answer to Q2\n",
    "# Continuation of answer to Q2\n",
    "\n",
    "# get numbers for correct and wrong predictions per class\n",
    "def confusionMatrix(labels, predictions, class1): \n",
    "    Act1Pred1 = 0\n",
    "    Act1Pred2 = 0\n",
    "    Act2Pred2 = 0\n",
    "    Act2Pred1 = 0\n",
    "    i = 0\n",
    "    while i<len(labels):\n",
    "        if labels[i] == class1:\n",
    "            if predictions[i] == class1:\n",
    "                Act1Pred1 += 1            # class 1 elements that have been predicted correctly\n",
    "            else:\n",
    "                Act1Pred2 += 1            # class 1 elements that have been predicted incorrectly\n",
    "        else:\n",
    "            if predictions[i] == class1:\n",
    "                Act2Pred1 += 1            # class 2 elements that have been predicted incorrectly\n",
    "            else:\n",
    "                Act2Pred2 += 1            # class 2 elements that have been predicted correctly\n",
    "        i += 1\n",
    "    return Act1Pred1, Act1Pred2, Act2Pred2, Act2Pred1\n",
    "\n",
    "def drawMatrix(matrixValues):\n",
    "    \n",
    "    print '\\t' + '\\t' + 'Pred.Class1' + '\\t' + 'Pred.Class2'\n",
    "    print 'Act.Class1' + '\\t' + '\\t' + repr(matrixValues[0]) + '\\t' + '\\t' + repr(matrixValues[1])\n",
    "    print 'Act.Class2' + '\\t' + '\\t' + repr(matrixValues[3]) + '\\t' + '\\t' + repr(matrixValues[2])\n",
    "    \n",
    "    #t.add_rows([['', 'Pred.: Class1', 'Pred.: Class2'], ['Act.: Class1', repr(matrixValues[0]), repr(matrixValues[1])], ['Act.: Class2', repr(matrixValues[3]), repr(matrixValues[2])]])\n",
    "    #print t.draw()\n",
    "\n",
    "# get precision: ratio of actual class c elements to predicted class c elements\n",
    "# '0.0+' is added to ensure it is stored as a float, not an integer, to avoid rounding errors\n",
    "def precision(ActCPredC, ActElsePredC):\n",
    "    return (0.0+ActCPredC)/(ActElsePredC+ActCPredC)\n",
    "\n",
    "# get recall: \n",
    "def recall(ActCPredC, ActCPredElse):\n",
    "    return (0.0+ActCPredC)/(ActCPredElse+ActCPredC)\n",
    "    \n",
    "# get accuracy: \n",
    "def accuracy(ActCPredC, ActElsePredElse, ActCPredElse, ActElsePredC):\n",
    "    return (0.0+ActCPredC+ActElsePredElse)/(ActCPredC+ActElsePredElse+ActCPredElse+ActElsePredC)\n",
    "\n",
    "\n",
    "def getStatistics(trees, subs, initialLabelList):\n",
    "    \"\"\"takes constructed trees, subsets, and an initial label list as input\n",
    "    subs: subsets with the structure: \n",
    "    subs[i] -> i=number of k-folds, subs[i][j] -> j=2 training+test set, subs[i][j][k] -> k=2 data+labels, subs[i][j][k][l] -> l=actual data points\"\"\"\n",
    "    \n",
    "    class1 = listValues(initialLabelList)[0]            # define class1 value as first value of label list of initial data set\n",
    "    # averages class 1\n",
    "    averagePrec1 = 0.0\n",
    "    averageRec1 = 0.0\n",
    "    # averages class 2\n",
    "    averagePrec2 = 0.0\n",
    "    averageRec2 = 0.0\n",
    "    # average accuracy (accuracy is not w.r.t. a specific class)\n",
    "    averageAcc = 0.0\n",
    "    \n",
    "    # from here on everything concerns ONE specific subset\n",
    "    a = 0\n",
    "    while a<len(subs):\n",
    "        da = subs[a][1][0]\n",
    "        la = subs[a][1][1]\n",
    "        testSetSize = len(da)\n",
    "        prediction = []\n",
    "    \n",
    "    \n",
    "        # predict class labels in test set for each test data point\n",
    "        i = 0\n",
    "        while i<testSetSize:\n",
    "            prediction.append(trees[a].classify(da[i]))\n",
    "            i += 1\n",
    "            \n",
    "        # get confusion matrix\n",
    "        matrix = confusionMatrix(la, prediction, class1)\n",
    "        #print matrix\n",
    "        \n",
    "        # get precision for class 1\n",
    "        prec = precision(matrix[0], matrix[3])\n",
    "        averagePrec1 += prec\n",
    "        #print prec\n",
    "        # get precision for class 2\n",
    "        prec = precision(matrix[2], matrix[1])\n",
    "        averagePrec2 += prec\n",
    "        #print prec\n",
    "        \n",
    "        # get recall for class 1\n",
    "        rec = recall(matrix[0], matrix[1])\n",
    "        averageRec1 += rec\n",
    "        #print rec\n",
    "        # get recall for class 2\n",
    "        rec = recall(matrix[2], matrix[3])\n",
    "        averageRec2 += rec\n",
    "        #print rec\n",
    "        \n",
    "        # get accuracy\n",
    "        acc = accuracy(matrix[0], matrix[1], matrix[2], matrix[3])\n",
    "        averageAcc += acc\n",
    "        #print acc\n",
    "        \n",
    "        a+=1\n",
    "        \n",
    "    averagePrec1 /= len(subsets) \n",
    "    averageRec1 /= len(subsets)\n",
    "    averagePrec2 /= len(subsets) \n",
    "    averageRec2 /= len(subsets)\n",
    "\n",
    "    averageAcc /= len(subsets)\n",
    "\n",
    "    # average precisions for class 1 and for class 2\n",
    "    print 'precision class1: ' + repr(averagePrec1) \n",
    "    print 'precision class2: ' + repr(averagePrec2)\n",
    "    # average recalls for class 1 and for class 2\n",
    "    print 'recall class1: ' + repr(averageRec1)\n",
    "    print 'recall class2: ' + repr(averageRec2)\n",
    "    # average accuracy\n",
    "    print 'accuracy: ' + repr(averageAcc)\n",
    "    \n",
    "getStatistics(roots[0], roots[1], l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer to Q3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different types of decisions\n",
    "\n",
    "Our tree for now can only make decisions based on binary conditions, but our dataset contains features with more than two possible values. \n",
    "\n",
    "**Question 4 [10 credits]** Implement a new type of node, ```MultiNode``` that has a child per possible value of the feature. Be careful to deal with the possibility that the test data may contain categories that the training set didn't have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code must be modified\n",
    "\n",
    "class MultiNode(Node):\n",
    "    \"\"\"Make a decision based the value of the given feature, i.e. as many children as there are \n",
    "    possible values for the feature\"\"\"\n",
    "    def __init__(self, inputs, targets, feature):\n",
    "        # YOUR CODE COMES HERE\n",
    "        self.feature = feature\n",
    "        self.value   = []                                   #each child would have its own value\n",
    "        self.IG      = 0.0\n",
    "        self.H       = entropy(targets)                     # Entropy of unsplit data\n",
    "        self.inputs  = inputs\n",
    "        self.targets = targets\n",
    "        N = float(targets.size)                             # Total number of datapoints\n",
    "\n",
    "        childList = []                                      #more than 2 children now\n",
    "        conditionList = []                                  #each childre has a condition\n",
    "        weightedEntropy = 0.0\n",
    "        \n",
    "        for value in listValues(inputs[:,feature]):\n",
    "            trueIndices = inputs[:,feature]==value\n",
    "            if trueIndices.all():\n",
    "                continue\n",
    "            childList += [Leaf(inputs[trueIndices,:], targets[trueIndices])]\n",
    "            conditionList += [\"%s==%s\" % (fname[feature], value), \"%s!=%s\" % (fname[feature], value)]\n",
    "            self.value += [value]\n",
    "            Nt = float(childList[-1].targets.size)  #number of elements in trueIndices\n",
    "            Pt = Nt/N                              #probability of truth due to this child\n",
    "            weightedEntropy -= Pt *  childList[-1].entropy()\n",
    "        ig = self.H - weightedEntropy\n",
    "        self.IG = ig\n",
    "        self.children = childList\n",
    "        self.conditions = conditionList\n",
    "       \n",
    "        \n",
    "    def entropy(self):\n",
    "        return self.H\n",
    "    \n",
    "    def informationGain(self):\n",
    "        \"\"\"Return the entropy gain of splitting the given data according to \n",
    "        the specified feature and value\"\"\"\n",
    "        return self.IG\n",
    "    \n",
    "    def classify(self, x):\n",
    "        # YOUR CODE COMES HERE\n",
    "        for i in range(0, len(self.value)):\n",
    "            if x[self.feature] == self.value:\n",
    "                return self.childList[i].classify(x)\n",
    "        return self.value       #this is what it will return for unseen data while testing\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"=> n-ary decision: %s = ?\" % (fname[self.feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with heterogeneous features\n",
    "\n",
    "The code below keeps a list of the type of node that is applicable for each feature, in this case ```BinaryNode``` or ```Multinode```. \n",
    "\n",
    "**Question 5 [5 credits]** Is a tree of multinodes better than a tree of binary nodes? Is it more compact? Is it generalising better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to Q5:*\n",
    "It is better because even though it increases the width (breadth/ spread of branches), The entropies are calculated at every step and it reduces the overall error of the leaf nodes. It may or may not reduce the complexity of the tree though. In this case it does not reduce the tree complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+->  => n-ary decision: stalk-shape = ?\n",
      "   +-> stalk-shape==ENLARGING => n-ary decision: gill-size = ?\n",
      "   |  +-> gill-size==BROAD => n-ary decision: gill-attachment = ?\n",
      "   |  |  +-> gill-attachment==ATTACHED \t\t\t=> EDIBLE\n",
      "   |  |  +-> gill-attachment!=FREE => n-ary decision: gill-spacing = ?\n",
      "   |  |     +-> gill-spacing==CLOSE => n-ary decision: Number-of-rings = ?\n",
      "   |  |     |  +-> Number-of-rings==NONE \t\t\t=> POISONOUS\n",
      "   |  |     |  +-> Number-of-rings!=NONE \t\t\t=> EDIBLE\n",
      "   |  |     |  +-> Number-of-rings!=ONE => n-ary decision: cap-shape = ?\n",
      "   |  |     |     +-> cap-shape==FLAT => n-ary decision: habitat = ?\n",
      "   |  |     |     |  +-> habitat==PATHS \t\t\t=> POISONOUS\n",
      "   |  |     |     |  +-> habitat!=PATHS \t\t\t=> POISONOUS\n",
      "   |  |     |     |  +-> habitat!=GRASSES \t\t\t=> POISONOUS\n",
      "   |  |     |     +-> cap-shape!=FLAT => n-ary decision: habitat = ?\n",
      "   |  |     |     |  +-> habitat==GRASSES \t\t\t=> POISONOUS\n",
      "   |  |     |     |  +-> habitat!=GRASSES \t\t\t=> POISONOUS\n",
      "   |  |     |     |  +-> habitat==WOODS \t\t\t=> EDIBLE\n",
      "   |  |     |     |  +-> habitat!=PATHS \t\t\t=> POISONOUS\n",
      "   |  |     |     +-> cap-shape!=BELL \t\t\t=> EDIBLE\n",
      "   |  |     +-> gill-spacing!=CROWDED \t\t\t=> EDIBLE\n",
      "   |  +-> gill-size!=NARROW => n-ary decision: stalk-colour-above-ring = ?\n",
      "   |     +-> stalk-colour-above-ring==WHITE => n-ary decision: gill-spacing = ?\n",
      "   |     |  +-> gill-spacing==CLOSE => n-ary decision: stalk-surface-above-ring = ?\n",
      "   |     |  |  +-> stalk-surface-above-ring==SILKY \t\t\t=> POISONOUS\n",
      "   |     |  |  +-> stalk-surface-above-ring!=SMOOTH \t\t\t=> POISONOUS\n",
      "   |     |  +-> gill-spacing!=CROWDED \t\t\t=> POISONOUS\n",
      "   |     +-> stalk-colour-above-ring!=YELLOW \t\t\t=> POISONOUS\n",
      "   +-> stalk-shape!=TAPERING => n-ary decision: stalk-colour-above-ring = ?\n",
      "      +-> stalk-colour-above-ring==PINK => n-ary decision: cap-shape = ?\n",
      "      |  +-> cap-shape==FLAT => n-ary decision: cap-colour = ?\n",
      "      |  |  +-> cap-colour==BROWN \t\t\t=> POISONOUS\n",
      "      |  |  +-> cap-colour!=BROWN \t\t\t=> POISONOUS\n",
      "      |  |  +-> cap-colour!=GRAY \t\t\t=> EDIBLE\n",
      "      |  +-> cap-shape!=FLAT \t\t\t=> POISONOUS\n",
      "      |  +-> cap-shape!=CONVEX => n-ary decision: cap-colour = ?\n",
      "      |     +-> cap-colour==BROWN \t\t\t=> POISONOUS\n",
      "      |     +-> cap-colour!=BROWN \t\t\t=> POISONOUS\n",
      "      |     +-> cap-colour!=GRAY \t\t\t=> EDIBLE\n",
      "      +-> stalk-colour-above-ring!=PINK \t\t\t=> EDIBLE\n",
      "      +-> stalk-colour-above-ring!=WHITE => n-ary decision: bruises? = ?\n",
      "         +-> bruises?==BRUISES => n-ary decision: cap-shape = ?\n",
      "         |  +-> cap-shape==FLAT \t\t\t=> EDIBLE\n",
      "         |  +-> cap-shape!=CONVEX \t\t\t=> EDIBLE\n",
      "         +-> bruises?!=NO => n-ary decision: cap-shape = ?\n",
      "            +-> cap-shape==FLAT => n-ary decision: stalk colour-below-ring = ?\n",
      "            |  +-> stalk colour-below-ring==PINK \t\t\t=> POISONOUS\n",
      "            |  +-> stalk colour-below-ring!=WHITE => n-ary decision: cap-surface = ?\n",
      "            |     +-> cap-surface==FIBROUS \t\t\t=> EDIBLE\n",
      "            |     +-> cap-surface!=FIBROUS \t\t\t=> EDIBLE\n",
      "            |     +-> cap-surface!=SCALY \t\t\t=> POISONOUS\n",
      "            +-> cap-shape!=FLAT \t\t\t=> POISONOUS\n",
      "            +-> cap-shape!=CONVEX => n-ary decision: stalk colour-below-ring = ?\n",
      "               +-> stalk colour-below-ring==PINK \t\t\t=> POISONOUS\n",
      "               +-> stalk colour-below-ring!=WHITE => n-ary decision: cap-surface = ?\n",
      "                  +-> cap-surface==FIBROUS \t\t\t=> EDIBLE\n",
      "                  +-> cap-surface!=FIBROUS \t\t\t=> EDIBLE\n",
      "                  +-> cap-surface!=SCALY \t\t\t=> POISONOUS\n"
     ]
    }
   ],
   "source": [
    "#This code is given but minSize is not implemented\n",
    "\n",
    "featureType = [ MultiNode for i in range(l.size) ]\n",
    "def c45(inputs, targets, minSize, featureType):\n",
    "    ig = 0.\n",
    "    res = None\n",
    "    for feature in range(inputs.shape[1]):\n",
    "        n = featureType[feature](inputs,targets,feature)\n",
    "        if n.informationGain() > ig:\n",
    "            ig = n.informationGain()\n",
    "            res = n\n",
    "    for i in range(len(res.children)):\n",
    "        if res.children[i].entropy() > 0. and res.children[i].targets.size>minSize:\n",
    "            res.children[i] = c45(res.children[i].inputs, res.children[i].targets, minSize, featureType)\n",
    "    return res\n",
    "\n",
    "root = c45(d,l,500, featureType)\n",
    "prettyPrint(root)\n",
    "\n",
    "\n",
    "##########\n",
    "#def c45(inputs, targets, minSize=100):\n",
    "#    ig = 0.\n",
    "#    res = None\n",
    "#    for feature in range(inputs.shape[1]):\n",
    "#        n = BinaryNode(inputs,targets,feature)\n",
    "#        if n.informationGain() > ig:\n",
    "#            ig = n.informationGain()\n",
    "#            res = n\n",
    "#    if not res:\n",
    "#        print \"WARNING: No informative feature found (Fishy!)\"\n",
    "#        return res\n",
    "#    print(len(res.children))\n",
    "#    for i in range(len(res.children)):\n",
    "#        if res.children[i].entropy() > 0. and res.children[i].targets.size>minSize:\n",
    "#            c = c45(res.children[i].inputs, res.children[i].targets, minSize)\n",
    "#            if c:\n",
    "#                res.children[i]=c\n",
    "#    return res\n",
    "\n",
    "#root = c45(d,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Playground for answering Q5\n",
    "######Question already answered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous values\n",
    "\n",
    "Until now, we have only dealt with categorical features. In the next part of the lab, you will deal with mixed categorical and continuous features.\n",
    "\n",
    "One way to deal with continuous features is to split the dataset based on whether the feature is larger than a certain threshold, or not. The number of possible threshold values to consider is, of course, infinite in theory, but when deciding what the threshold value should be, based on a training dataset, only threshold values that will result in a different classification of those training examples matter: those are values that are smaller than one datapoint's feature value and larger than another's. Moreover, when considering a threshold in between two feature values, any value of the threshold in that range will result in the same classification, but to maximise generalisation it makes sense to maximise the margin: that is, to take a threshold value that is in the middle between the two.\n",
    "\n",
    "Also note that it only makes sense to consider ranges of feature values that are disjoint, i.e., consider thresholds that are in the middle between neighbouring datapoints (along that feature's dimension), not in the middle between any pair of datapoints.\n",
    "\n",
    "**Question 6 [10 credits]**. You guessed it, implement a node ```ContNode``` that selects a threshold value based on its training subset and classifies new datapoints based on whether they are smaller than that value, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code must be modified.\n",
    "\n",
    "class ContNode(Node):\n",
    "    \"\"\"Make a binary decision based on a threshold, for the value of the given feature\"\"\"\n",
    "    def __init__(self, inputs, targets, feature):\n",
    "         # YOUR CODE COMES HERE\n",
    "        self.feature = feature\n",
    "        self.value   = None\n",
    "        self.IG      = 0.0\n",
    "        self.H       = entropy(targets)                     # Entropy of unsplit data\n",
    "        self.inputs  = inputs\n",
    "        self.targets = targets\n",
    "        N = float(targets.size)                             # Total number of datapoints\n",
    "        threshold = 0.0\n",
    "        i = 0\n",
    "        for value in listValues(targets):\n",
    "            targetIndicesTrue  = targets == value\n",
    "            threshold += np.mean(inputs[targetIndicesTrue,feature])    \n",
    "            i += 1\n",
    "        threshold = threshold/i\n",
    "        indicesTrue  = inputs[:,feature] >= threshold\n",
    "        indicesFalse = inputs[:,feature] < threshold\n",
    "\n",
    "        if indicesTrue.all() or indicesFalse.all():\n",
    "            threshold = 0.0\n",
    "            ig =0.0\n",
    "        else:    \n",
    "            children = [ Leaf(inputs[indicesTrue,:], targets[indicesTrue]),\n",
    "                        Leaf(inputs[indicesFalse,:], targets[indicesFalse]) ]\n",
    "            conditions = [ \"%s>=%s\" % (fname[feature], value), \"%s<%s\" % (fname[feature], value) ]\n",
    "\n",
    "            Nt = float(children[0].targets.size)        # N.o. datapoints in True condition\n",
    "            Nf = float(children[1].targets.size)        # N.o. datapoints in False condition\n",
    "            pt = Nt/N\n",
    "            pf = Nf/N\n",
    "            ig = self.H - pt * children[0].entropy() - pf * children[1].entropy()\n",
    "        if ig > self.IG:\n",
    "            self.value = threshold\n",
    "            self.IG = ig\n",
    "            self.children = children\n",
    "            self.conditions = conditions\n",
    "    \n",
    "                \n",
    "    def entropy(self):\n",
    "        return self.H\n",
    "    \n",
    "    def informationGain(self):\n",
    "        \"\"\"Return the entropy gain of splitting the given data according to \n",
    "        the specified feature and value\"\"\"\n",
    "        return self.IG\n",
    "    \n",
    "    def classify(self, x):\n",
    "        if x[self.feature] >= self.value:\n",
    "            return self.children[0].classify(x)\n",
    "        else:\n",
    "            return self.children[1].classify(x)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"=> threshold decision: %s >= %f\" % (fname[self.feature],self.value)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification based on mixed features\n",
    "\n",
    "The following dataset contains 14 features describing people's situation, and the classification task is to predict whether they earn more than \\$50k a year or not. The categorical decision nodes we implemented above can work with any representation of categorical data, but the continuous decision node requires a numerical representation of the feature. Numpy arrays cannot easily contain both strings and numbers, and so to simplify our life, we converted the dataset to \n",
    "\n",
    "To allow our classifier to work with mixed features, we use a numerical representation of the categorical features in the following. So, let's first load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inputs', 'catindices', 'fnames', 'targets', 'ftypes']\n",
      "455854\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "\n",
    "income = np.load('income.npz')\n",
    "print income.keys()\n",
    "inputs = income['inputs']\n",
    "targets = income['targets']\n",
    "fname = income['fnames']\n",
    "ftypes = income['ftypes']\n",
    "print(np.size(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 [10 credits]** Take 1000 training datapoints at random from the dataset, and another 1000 test points that are distinct from those (don't mix train and test). Train a decision tree on the train set and report the confusion matrix obtained on the test set. What are the obtained precision, recall and accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: \n",
      "0.875748502994 0.612716763006\n",
      "recall: \n",
      "0.897239263804 0.560846560847\n",
      "accuracy: \n",
      "0.775267538644\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "import random\n",
    "def confusionMatrix(labels, predictions, class1): \n",
    "    Act1Pred1 = 0\n",
    "    Act1Pred2 = 0\n",
    "    Act2Pred2 = 0\n",
    "    Act2Pred1 = 0\n",
    "    i = 0\n",
    "    while i<testSetSize:\n",
    "        if labels[i] == class1:\n",
    "            if predictions[i] == class1:\n",
    "                Act1Pred1 += 1            # class 1 elements that have been predicted correctly\n",
    "            else:\n",
    "                Act1Pred2 += 1            # class 1 elements that have been predicted incorrectly\n",
    "        else:\n",
    "            if predictions[i] == class1:\n",
    "                Act2Pred1 += 1            # class 2 elements that have been predicted incorrectly\n",
    "            else:\n",
    "                Act2Pred2 += 1            # class 2 elements that have been predicted correctly\n",
    "        i += 1\n",
    "    return Act1Pred1, Act1Pred2, Act2Pred2, Act2Pred1\n",
    "\n",
    "# get precision: ratio of actual class c elements to predicted class c elements\n",
    "# '0.0+' is added to ensure it is stored as a float, not an integer, to avoid rounding errors\n",
    "def precision(ActCPredC, ActElsePredC):\n",
    "    return (0.0+ActCPredC)/(ActElsePredC+ActCPredC)\n",
    "\n",
    "# get recall: \n",
    "def recall(ActCPredC, ActCPredElse):\n",
    "    return (0.0+ActCPredC)/(ActCPredElse+ActCPredC)\n",
    "    \n",
    "# get accuracy: \n",
    "def accuracy(ActCPredC, ActElsePredElse, ActCPredElse, ActElsePredC):\n",
    "    return (0.0+ActCPredC+ActElsePredElse)/(ActCPredC+ActElsePredElse+ActCPredElse+ActElsePredC)\n",
    "\n",
    "featureType = [ ContNode if ftypes[i]=='continuous' else BinaryNode for i in range(len(ftypes)) ]\n",
    "#tree = c45(inputs[:10000,:],targets[:10000],100,featureType)\n",
    "testDataPoints = random.sample(range(10000), 1000)\n",
    "trainigDataPoints = (random.sample(range(20000,30000), 1000))\n",
    "testDataInputs = inputs[testDataPoints,:]\n",
    "testDataTargets = targets[testDataPoints]\n",
    "trainingDataInputs = inputs[trainigDataPoints,:]\n",
    "trainingDataTargets = targets[trainigDataPoints]\n",
    "\n",
    "#prettyPrint(tree)\n",
    "tree = c45(trainingDataInputs,trainingDataTargets,100,featureType)\n",
    "class1 = listValues(testDataTargets)[0]\n",
    "prediction = []\n",
    "i = 0\n",
    "while i< (1000):\n",
    "    prediction.append(tree.classify(testDataInputs[i]))\n",
    "    i += 1\n",
    "myMatrix = confusionMatrix(testDataTargets, prediction, class1)\n",
    "precisionClass1 = precision(myMatrix[0], myMatrix[3])\n",
    "precisionClass2 = precision(myMatrix[2], myMatrix[1])\n",
    "recallClass1 = recall(myMatrix[0], myMatrix[1])\n",
    "recallClass2 = recall(myMatrix[2], myMatrix[3])\n",
    "accuracyContNode = accuracy(myMatrix[0], myMatrix[1], myMatrix[2], myMatrix[3])\n",
    "print \"precision: \"\n",
    "print precisionClass1, precisionClass2\n",
    "print \"recall: \"\n",
    "print recallClass1, recallClass2\n",
    "print \"accuracy: \"\n",
    "print accuracyContNode\n",
    "#print(myMatrix)\n",
    "# Answer to Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
